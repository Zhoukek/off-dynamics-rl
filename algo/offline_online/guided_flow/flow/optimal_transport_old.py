import math
import warnings
from functools import partial
from typing import Optional, Union

import numpy as np
import ot as pot
import torch


class OTPlanSampler:
    """OTPlanSampler implements sampling coordinates according to an OT plan (wrt squared Euclidean
    cost) with different implementations of the plan calculation."""

    def __init__(
        self,
        method: str,
        reg: float = 0.05,
        reg_m: float = 1.0,
        normalize_cost: bool = False,
        num_threads: Union[int, str] = 'max',
        warn: bool = True,
    ) -> None:
        """Initialize the OTPlanSampler class.

        Parameters
        ----------
        method: str
            choose which optimal transport solver you would like to use.
            Currently supported are ["exact", "sinkhorn", "unbalanced",
            "partial"] OT solvers.
        reg: float, optional
            regularization parameter to use for Sinkhorn-based iterative solvers.
        reg_m: float, optional
            regularization weight for unbalanced Sinkhorn-knopp solver.
        normalize_cost: bool, optional
            normalizes the cost matrix so that the maximum cost is 1. Helps
            stabilize Sinkhorn-based solvers. Should not be used in the vast
            majority of cases.
        num_threads: int or str, optional
            number of threads to use for the "exact" OT solver. If "max", uses
            the maximum number of threads.
        warn: bool, optional
            if True, raises a warning if the algorithm does not converge
        """
        # ot_fn should take (a, b, M) as arguments where a, b are marginals and
        # M is a cost matrix
        if method == "exact":
            self.ot_fn = partial(pot.emd, numThreads=num_threads)
        elif method == "sinkhorn":
            self.ot_fn = partial(pot.sinkhorn, reg=reg)
        elif method == "unbalanced":
            self.ot_fn = partial(pot.unbalanced.sinkhorn_knopp_unbalanced, reg=reg, reg_m=reg_m)
        elif method == "partial":
            self.ot_fn = partial(pot.partial.entropic_partial_wasserstein, reg=reg)
        else:
            raise ValueError(f"Unknown method: {method}")
        self.reg = reg
        self.reg_m = reg_m
        self.normalize_cost = normalize_cost
        self.warn = warn

    def get_map(self, x0, x1):
        """Compute the OT plan (wrt squared Euclidean cost) between a source and a target
        minibatch.

        Parameters
        ----------
        x0 : Tensor, shape (bs, *dim)
            represents the source minibatch
        x1 : Tensor, shape (bs, *dim)
            represents the source minibatch

        Returns
        -------
        p : numpy array, shape (bs, bs)
            represents the OT plan between minibatches
        """
        a, b = pot.unif(x0.shape[0]), pot.unif(x1.shape[0])
        if x0.dim() > 2:
            x0 = x0.reshape(x0.shape[0], -1)
        if x1.dim() > 2:
            x1 = x1.reshape(x1.shape[0], -1)
        M = torch.cdist(x0, x1) ** 2
        if self.normalize_cost:
            M = M / M.max()  # should not be normalized when using minibatches
        p = self.ot_fn(a, b, M.detach().cpu().numpy())
        if not np.all(np.isfinite(p)):
            print("ERROR: p is not finite")
            print(p)
            print("Cost mean, max", M.mean(), M.max())
            print(x0, x1)
        if np.abs(p.sum()) < 1e-8:
            if self.warn:
                warnings.warn("Numerical errors in OT plan, reverting to uniform plan.")
            p = np.ones_like(p) / p.size
        return p
        
    def get_map_condition_version(
        self,
        x0: torch.Tensor, x1: torch.Tensor,
        s0: torch.Tensor, s1: torch.Tensor,
        a0: torch.Tensor, a1: torch.Tensor,
        eta: float = 0.0,
    ) -> np.ndarray:
        """
        Compute OT plan with cost = ||x0 - x1||^2 + lam * ( ||s0 - s1||^2 + ||a0 - a1||^2 ).
        """
        bs0, bs1 = x0.shape[0], x1.shape[0]
        # uniform marginals
        a, b = pot.unif(bs0), pot.unif(bs1)

        # flatten features if needed
        def _flatten(t):
            return t.reshape(t.shape[0], -1) if t.dim() > 2 else t

        x0f, x1f = _flatten(x0), _flatten(x1)
        s0f, s1f = _flatten(s0), _flatten(s1)
        a0f, a1f = _flatten(a0), _flatten(a1)

        # pairwise L2 distances (not squared)
        Mx = torch.cdist(x0f, x1f, p=2) ** 2              # [bs0, bs1]
        Ms = torch.cdist(s0f, s1f, p=2) ** 2              # [bs0, bs1]
        Ma = torch.cdist(a0f, a1f, p=2) ** 2              # [bs0, bs1]

        M = Mx + eta * (Ms + Ma)

        if self.normalize_cost:
            maxv = M.max()
            if torch.isfinite(maxv) and maxv > 0:
                M = M / maxv

        # P = self.ot_fn(a, b, M.detach().cpu().numpy())
    
        # if not np.all(np.isfinite(P)):
        #     print("ERROR: OT plan contains non-finite values.")
        #     print("Cost mean, max:", float(M.mean()), float(M.max()))
        #     if self.warn:
        #         warnings.warn("Non-finite OT plan; reverting to uniform.")
        #     P = np.ones_like(P) / P.size

        # if abs(P.sum()) < 1e-12:
        #     if self.warn:
        #         warnings.warn("Degenerate OT plan; reverting to uniform.")
        #     P = np.ones_like(P) / P.size

        # use sinkhorn
        # a, b -> GPU torch
        a_tensor = torch.from_numpy(a).cuda() if isinstance(a, np.ndarray) else a.cuda()
        b_tensor = torch.from_numpy(b).cuda() if isinstance(b, np.ndarray) else b.cuda()
        # 确保数据类型一致
        a_tensor = a_tensor.float()  # 转换为 float32
        b_tensor = b_tensor.float()
        M = M.float()

        # Sinkhorn OT (GPU, torch)
        P = self.ot_fn(a_tensor, b_tensor, M)   # torch.Tensor on GPU

        # ---------- 数值稳定性检查（torch 里完成） ----------
        if not torch.isfinite(P).all():
            print("ERROR: OT plan contains non-finite values.")
            print("Cost mean, max:", float(M.mean()), float(M.max()))
            if self.warn:
                warnings.warn("Non-finite OT plan; reverting to uniform.")

            P = torch.ones_like(P)
            P = P / P.numel()

        # ---------- 退化解检查 ----------
        if torch.abs(P.sum()) < 1e-12:
            if self.warn:
                warnings.warn("Degenerate OT plan; reverting to uniform.")

            P = torch.ones_like(P)
            P = P / P.numel()

        # ---------- 最后一步：转回 numpy ----------
        P = P.detach().cpu().numpy()

        return P



    def sample_map(self, pi, batch_size, replace=True):
        r"""Draw source and target samples from pi  $(x,z) \sim \pi$

        Parameters
        ----------
        pi : numpy array, shape (bs, bs)
            represents the source minibatch
        batch_size : int
            represents the OT plan between minibatches
        replace : bool
            represents sampling or without replacement from the OT plan

        Returns
        -------
        (i_s, i_j) : tuple of numpy arrays, shape (bs, bs)
            represents the indices of source and target data samples from $\pi$
        """
        p = pi.flatten()
        p = p / p.sum()
        choices = np.random.choice(
            pi.shape[0] * pi.shape[1], p=p, size=batch_size, replace=replace
        )
        return np.divmod(choices, pi.shape[1])

    def sample_plan(self, x0, x1, replace=True, batch_size=None):
        r"""Compute the OT plan $\pi$ (wrt squared Euclidean cost) between a source and a target
        minibatch and draw source and target samples from pi $(x,z) \sim \pi$

        Parameters
        ----------
        x0 : Tensor, shape (bs, *dim)
            represents the source minibatch
        x1 : Tensor, shape (bs, *dim)
            represents the source minibatch
        replace : bool
            represents sampling or without replacement from the OT plan

        Returns
        -------
        x0[i] : Tensor, shape (bs, *dim)
            represents the source minibatch drawn from $\pi$
        x1[j] : Tensor, shape (bs, *dim)
            represents the source minibatch drawn from $\pi$
        """
        pi = self.get_map(x0, x1)
        if batch_size is None:
            batch_size = x0.shape[0]
        i, j = self.sample_map(pi, batch_size, replace=replace)
        return x0[i], x1[j]

    def sample_plan_condition_version(self, x0, x1, s0, s1, a0, a1, replace=True, batch_size=None):
        r"""Compute the OT plan $\pi$ (wrt squared Euclidean cost) between a source and a target
        minibatch and draw source and target samples from pi $(x,z) \sim \pi$

        Parameters
        ----------
        x0 : Tensor, shape (bs, *dim)
            represents the source minibatch
        x1 : Tensor, shape (bs, *dim)
            represents the source minibatch
        s0 : Tensor, shape (bs, *dim)
            represents the source label minibatch
        s1 : Tensor, shape (bs, *dim)
            represents the target label minibatch
        a0 : Tensor, shape (bs, *dim)
            represents the source action minibatch
        a1 : Tensor, shape (bs, *dim)
            represents the target action minibatch
        replace : bool
            represents sampling or without replacement from the OT plan

        Returns
        -------
        x0[i] : Tensor, shape (bs, *dim)
            represents the source minibatch drawn from $\pi$
        x1[j] : Tensor, shape (bs, *dim)
            represents the source minibatch drawn from $\pi$
        """
        pi = self.get_map_condition_version(x0, x1, s0, s1, a0, a1)
        if batch_size is None:
            batch_size = x0.shape[0]
        i, j = self.sample_map(pi, batch_size, replace=replace)
        return x0[i], x1[j]




    def sample_plan_with_labels(self, x0, x1, y0=None, y1=None, replace=True, batch_size=None):
        r"""Compute the OT plan $\pi$ (wrt squared Euclidean cost) between a source and a target
        minibatch and draw source and target labeled samples from pi $(x,z) \sim \pi$

        Parameters
        ----------
        x0 : Tensor, shape (bs, *dim)
            represents the source minibatch
        x1 : Tensor, shape (bs, *dim)
            represents the target minibatch
        y0 : Tensor, shape (bs)
            represents the source label minibatch
        y1 : Tensor, shape (bs)
            represents the target label minibatch
        replace : bool
            represents sampling or without replacement from the OT plan

        Returns
        -------
        x0[i] : Tensor, shape (bs, *dim)
            represents the source minibatch drawn from $\pi$
        x1[j] : Tensor, shape (bs, *dim)
            represents the target minibatch drawn from $\pi$
        y0[i] : Tensor, shape (bs, *dim)
            represents the source label minibatch drawn from $\pi$
        y1[j] : Tensor, shape (bs, *dim)
            represents the target label minibatch drawn from $\pi$
        """
        pi = self.get_map(x0, x1)
        if batch_size is None:
            batch_size = x0.shape[0]
        i, j = self.sample_map(pi, x0.shape[0], replace=replace)
        return (
            x0[i],
            x1[j],
            y0[i] if y0 is not None else None,
            y1[j] if y1 is not None else None,
        )

    def sample_trajectory(self, X):
        """Compute the OT trajectories between different sample populations moving from the source
        to the target distribution.

        Parameters
        ----------
        X : Tensor, (bs, times, *dim)
            different populations of samples moving from the source to the target distribution.

        Returns
        -------
        to_return : Tensor, (bs, times, *dim)
            represents the OT sampled trajectories over time.
        """
        times = X.shape[1]
        pis = []
        for t in range(times - 1):
            pis.append(self.get_map(X[:, t], X[:, t + 1]))

        indices = [np.arange(X.shape[0])]
        for pi in pis:
            j = []
            for i in indices[-1]:
                j.append(np.random.choice(pi.shape[1], p=pi[i] / pi[i].sum()))
            indices.append(np.array(j))

        to_return = []
        for t in range(times):
            to_return.append(X[:, t][indices[t]])
        to_return = np.stack(to_return, axis=1)
        return to_return


def wasserstein(
    x0: torch.Tensor,
    x1: torch.Tensor,
    method: Optional[str] = None,
    reg: float = 0.05,
    power: int = 2,
    **kwargs,
) -> float:
    """Compute the Wasserstein (1 or 2) distance (wrt Euclidean cost) between a source and a target
    distributions.

    Parameters
    ----------
    x0 : Tensor, shape (bs, *dim)
        represents the source minibatch
    x1 : Tensor, shape (bs, *dim)
        represents the source minibatch
    method : str (default : None)
        Use exact Wasserstein or an entropic regularization
    reg : float (default : 0.05)
        Entropic regularization coefficients
    power : int (default : 2)
        power of the Wasserstein distance (1 or 2)
    Returns
    -------
    ret : float
        Wasserstein distance
    """
    assert power == 1 or power == 2
    # ot_fn should take (a, b, M) as arguments where a, b are marginals and
    # M is a cost matrix
    if method == "exact" or method is None:
        ot_fn = pot.emd2
    elif method == "sinkhorn":
        ot_fn = partial(pot.sinkhorn2, reg=reg)
    else:
        raise ValueError(f"Unknown method: {method}")

    a, b = pot.unif(x0.shape[0]), pot.unif(x1.shape[0])
    if x0.dim() > 2:
        x0 = x0.reshape(x0.shape[0], -1)
    if x1.dim() > 2:
        x1 = x1.reshape(x1.shape[0], -1)
    M = torch.cdist(x0, x1)
    if power == 2:
        M = M**2
    ret = ot_fn(a, b, M.detach().cpu().numpy(), numItermax=int(1e7))
    if power == 2:
        ret = math.sqrt(ret)
    return ret